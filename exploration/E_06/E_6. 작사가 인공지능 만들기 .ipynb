{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "norwegian-vertex",
   "metadata": {},
   "source": [
    "이번 노드에서는 인공지능이 문장을 이해하는 방식과 작문을 가르치는 법을 배울 것입니다. \n",
    "\n",
    "시퀀스 데이터가 어느정도는 연관성이 있어줘야 합니다. \n",
    "문장을 구성하는 각 단어들은 문법이라는 규칙을 따라 배열되어 있습니다. \n",
    "문법을 기반으로 예측하면 어려우니 주어 동사 목적어 순으로 오도록 해라..이거보다 통계를 기반하여 다음으로 올 단어를 예측해 보도록하겠습니다. \n",
    "i 다음에 am 을 쓰면 반 이상은 맞더라. 대체로~~~라고 쓰여있어서 저도 대체로~~라고 읽어보았습니다. \n",
    "\n",
    "인공지능에세 수많은 글을 읽게 하고 나는 밥을 다음에 먹는다가 대체로 많이 나왔다...라는 것을 알 수 있게 됩니다. \n",
    "이 방식을 가장 잘 처리한 것이 인공지능 중 하나인 rnn 순환신경망입니다. \n",
    "간단히 구조를 살펴보면요!\n",
    "\n",
    "나는을 생성하고, 생성한 단어를 다시 입력으로 사용하고 아웃풋을 다시 입력값으로 사용하기에 순환한다해서 순환신경망이라고 볼 수 있습니다. \n",
    "이렇게 하고 다 만들면 다 만들었다 라는 사인으로 end라는 특수 토큰을 생성합니다. \n",
    "\n",
    "\n",
    "언어모델이라는 것은 n개의 단어가 주어졌을 때 n+1개 일때 wn+1로 무엇이 올지를 예측하는 확률 모델을 언어모델이라고 합니다. \n",
    "확률이 높다는 것은 자연스럽다는 의미입니다. \n",
    "처음에 나는은 어떻게 생성을 하는가요?\n",
    "<start>라는 것을 써서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rnn_1 = tf.keras.layers.LSTM(hidden_size, retrun_sequences=True)\n",
    "self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
